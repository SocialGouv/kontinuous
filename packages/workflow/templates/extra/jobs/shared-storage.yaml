{{ if and .Values.global.extra.jobs.enabled .Values.global.extra.jobs.sharedStorage.enabled }}
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jobs-shared-storage
  annotations: {}
  labels: {}
  namespace: {{ .Values.global.jobNamespace }}
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: {{ .Values.global.extra.jobs.sharedStorage.size }}
  storageClassName: {{ .Values.global.extra.jobs.sharedStorage.className }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: jobci-clean-workflow
  namespace: {{ .Values.global.jobNamespace }}
  annotations: {}
spec:
  schedule: "0 0 * * *" # Run once a day at midnight
  # schedule: "* * * * *" # Run at every minute
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: job
              image: bitnami/kubectl:latest
              imagePullPolicy: IfNotPresent
              envFrom:
              - secretRef:
                  name: kubeconfig
              command:
                - /bin/bash
                - -c
                - |
                  echo "$KUBECONFIG" > ~/.kube/config
                  export KUBECONFIG=~/.kube/config
                  export JOB_NAMESPACE="{{ .Values.global.jobNamespace }}"
                  ACTIVE_JOBS=$(kubectl get jobs -n $JOB_NAMESPACE -o json | jq ".items | .[] | select(.status.active > 0).metadata.name")
                  for subdir in /workflow/*; do
                    [ -d "$subdir" ] || continue
                    subdir_basename=${subdir##*/}
                    if ! echo $ACTIVE_JOBS | grep -e "job-${subdir_basename}"; then
                      echo "Cleaning '$subdir'"
                      rm -rf $subdir
                    fi
                  done
              volumeMounts:
                - name: workflow
                  mountPath: /workflow

          volumes:
            - name: workflow
              persistentVolumeClaim:
                claimName: jobs-shared-storage
{{ end }}